precision[i] = sum(prediccion_knn_cv == df_train_knn$RainTomorrow & prediccion_knn_cv == 1) / sum(prediccion_knn_cv == 1)
f1score[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i])
}
resultados_knn = as.data.frame(cbind(accuracy,f1score,precision,recall))
resultados_knn = resultados_knn %>% mutate(index=as.factor(seq(1:long)))
print(paste('Máximo resultado de Accuracy', max(resultados_knn$accuracy)))
print(paste('Número de vecinos usados', which.max(resultados_knn$accuracy)))
df_train$RainTomorrow<-as.factor(df_train$RainTomorrow)
df_train_esc = df_train
df_test_esc = df_test
preprocessParams <- preProcess(df_train_esc[numeric_vars], method=c("center", "scale"))
df_train_esc[numeric_vars] <- predict(preprocessParams, df_train_esc[numeric_vars])
df_test_esc[numeric_vars] <- predict(preprocessParams, df_test_esc[numeric_vars])
df_train_knn = df_train_esc[ , -which(names(df_train_esc) %in% c("Location","Season", 'WindGustDir', 'WindDir3pm', 'WindDir9am', 'Cloud9am', 'Cloud3pm'))]
df_test_knn = df_test_esc[ , -which(names(df_test_esc) %in% c("Location","Season", 'WindGustDir', 'WindDir3pm', 'WindDir9am', 'Cloud9am', 'Cloud3pm'))]
# PREPARACIÓN DATASET
x_train = df_train_knn[ , -which(names(df_train_knn) %in% c( 'RainTomorrow'))]
x_test = df_test_knn[ , -which(names(df_test_knn) %in% c( 'RainTomorrow'))]
long = 15
accuracy = rep(0,long)
f1score = rep(0,long)
recall = rep(0,long)
precision = rep(0,long)
for (i in 1:long)
{
prediccion_knn_cv =knn.cv(x_train, k=i, cl=df_train_knn$RainTomorrow)
accuracy[i] = sum(prediccion_knn_cv == df_train_knn$RainTomorrow) /nrow(df_train)
recall[i] = sum(prediccion_knn_cv == df_train_knn$RainTomorrow & df_train$RainTomorrow == 1) / sum(df_train$RainTomorrow == 1)
precision[i] = sum(prediccion_knn_cv == df_train_knn$RainTomorrow & prediccion_knn_cv == 1) / sum(prediccion_knn_cv == 1)
f1score[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i])
}
resultados_knn = as.data.frame(cbind(accuracy,f1score,precision,recall))
resultados_knn = resultados_knn %>% mutate(index=as.factor(seq(1:long)))
print(paste('Máximo resultado de Accuracy', max(resultados_knn$accuracy)))
print(paste('Número de vecinos usados', which.max(resultados_knn$accuracy)))
ggplot(data=resultados_knn,aes(x=index,y=accuracy)) +
geom_col(colour="cyan4",fill="cyan")+
ggtitle("Accuracy")
#ggplot(data=resultados_knn,aes(x=index,y=recall)) +
#  geom_col(colour="darkblue",fill="blue") +
#  ggtitle("Recall values")
best_prediction_knn =knn(x_train, x_test, k=14, cl=df_train_knn$RainTomorrow, prob=TRUE)
#which.max(resultados_knn$accuracy)
conf_table = table(best_prediction_knn, df_test_knn$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1'))
best_prediction_knn =knn(x_train, x_test, k=which.max(resultados_knn$accuracy), cl=df_train_knn$RainTomorrow, prob=TRUE)
#which.max(resultados_knn$accuracy)
conf_table = table(best_prediction_knn, df_test_knn$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1'))
variables_rf
new_train = df_train_d[,(names(df_train_d) %in% c(variables_rf,'RainTomorrow'))]
down_train = downsample(new_train)
best_prediction_knn =knn(x_train, x_test, k=which.max(resultados_knn$accuracy), cl=df_train_knn$RainTomorrow, prob=TRUE)
conf_table = table(best_prediction_knn, df_test_knn$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
NBclassfier=naiveBayes(RainTomorrow~., data=down_train)
pred <- predict(NBclassfier, df_test)
conf_table = table(pred, df_test$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
conf_table_glm = table(best_glm_results, new_test_glm$RainTomorrow)
cm_glm = caret::confusionMatrix(conf_table_glm, positive = '1')
conf_table_svm = table(best_prediction_svm, new_test_svm$RainTomorrow)
cm_svm = caret::confusionMatrix(conf_table_svm, positive = '1')
conf_table_tree = table(pred_tree, df_test_d$RainTomorrow)
cm_tree = caret::confusionMatrix(conf_table_tree, positive = '1')
conf_table_rf = table(pred_forest, new_test_rf$RainTomorrow)
cm_rf = caret::confusionMatrix(conf_table_rf, positive = '1')
conf_table_knn = table(best_prediction_knn, df_test_knn$RainTomorrow)
cm_knn = caret::confusionMatrix(conf_table_knn, positive = '1')
conf_table_nb = table(pred, df_test$RainTomorrow)
cm_nb = caret::confusionMatrix(conf_table_nb, positive = '1')
model_compare <- data.frame(Model = c('RegresiónLogística',
'RF',
'Decision Tree',
'KNN',
'SVM',
'NaiveBayes '),
Accuracy = c(cm_glm$overall[1],
cm_rf$overall[1],
cm_tree$overall[1],
cm_knn$overall[1],
cm_svm$overall[1],
cm_nb$overall[1]))
ggplot(aes(x=Model, y=Accuracy), data=model_compare) +
geom_bar(stat='identity') +
ggtitle('Comparación del accuracy según cada modelo') +
xlab('Models') +
ylab('Overall Accuracy')
espec_svm = cm_svm$table[2,2]/(cm_svm$table[2,2] + cm_svm$table[1,2])
espec_glm = cm_glm$table[2,2]/(cm_glm$table[2,2] + cm_glm$table[1,2])
espec_svm = cm_svm$table[2,2]/(cm_svm$table[2,2] + cm_svm$table[1,2])
espec_tree = cm_tree$table[2,2]/(cm_tree$table[2,2] + cm_tree$table[1,2])
espec_rf = cm_rf$table[2,2]/(cm_rf$table[2,2] + cm_rf$table[1,2])
espec_knn = cm_knn$table[2,2]/(cm_knn$table[2,2] + cm_knn$table[1,2])
espec_nb = cm_nb$table[2,2]/(cm_nb$table[2,2] + cm_nb$table[1,2])
model_compare <- data.frame(Model = c('RegresiónLogística',
'RF',
'Decision Tree',
'KNN',
'SVM',
'NaiveBayes '),
sensibilidad = c(espec_glm,
espec_rf,
espec_tree,
espec_knn,
espec_svm,
espec_nb))
ggplot(aes(x=Model, y=sensibilidad), data=model_compare) +
geom_bar(stat='identity') +
ggtitle('Comparación de la Sensibilidad según cada modelo') +
xlab('Models') +
ylab('Overall Recall')
model_compare
Precision_glm = cm_glm$table[1,1]/(cm_glm$table[1,1] + cm_glm$table[2,1])
Precision_svm = cm_svm$table[1,1]/(cm_svm$table[1,1] + cm_svm$table[2,1])
Precision_tree = cm_tree$table[1,1]/(cm_tree$table[1,1] + cm_tree$table[2,1])
Precision_rf = cm_rf$table[1,1]/(cm_rf$table[1,1] + cm_rf$table[2,1])
Precision_knn = cm_knn$table[1,1]/(cm_knn$table[1,1] + cm_knn$table[2,1])
Precision_nb = cm_nb$table[1,1]/(cm_nb$table[1,1] + cm_nb$table[2,1])
Fscore_glm = (2*Precision_glm*espec_glm) / sum(Precision_glm, espec_glm)
Fscore_svm = (2*Precision_svm*espec_svm) / sum(Precision_svm, espec_svm)
Fscore_tree = (2*Precision_tree*espec_tree) / sum(Precision_tree, espec_tree)
Fscore_rf = (2*Precision_rf*espec_rf) / sum(Precision_rf, espec_rf)
Fscore_knn = (2*Precision_knn*espec_knn) / sum(Precision_knn, espec_knn)
Fscore_nb = (2*Precision_nb*espec_nb) / sum(Precision_nb, espec_nb)
model_compare <- data.frame(Model = c('RegresiónLogística',
'RF',
'Decision Tree',
'KNN',
'SVM',
'NaiveBayes '),
F1 = c(Fscore_glm,
Fscore_rf,
Fscore_tree,
Fscore_knn,
Fscore_svm,
Fscore_nb))
ggplot(aes(x=Model, y=F1), data=model_compare) +
geom_bar(stat='identity') +
ggtitle('Comparación de F1 score según cada modelo') +
xlab('Models') +
ylab('Overall F1')
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=200)
plot(forest$err.rate[, 1], type = "l", xlab = "Trees", ylab = "error") #error
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=200)
#prediction
new_test_rf = df_test_cat[,(names(df_test_cat) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
varImpPlot(forest)
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=400)
plot(forest$err.rate[, 1], type = "l", xlab = "Trees", ylab = "error") #error
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=300)
#prediction
new_test_rf = df_test_cat[,(names(df_test_cat) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
varImpPlot(forest)
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
fit_rf = randomForest(RainTomorrow~., data=df_train_d)
peso_variables_rf = importance(fit_rf)
feat_imp_df <- importance(fit_rf) %>%
data.frame() %>%
mutate(feature = row.names(.))
variables_seleccionadas = feat_imp_df %>% filter(MeanDecreaseGini> 100)
variables_rf = variables_seleccionadas[['feature']]
varImpPlot(fit_rf)
print(variables_rf)
new_train_rf = df_train_d[,(names(df_train_d) %in% c(variables_rf,'RainTomorrow'))]
down_train_rf = downsample(new_train_rf)
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=400)
plot(forest$err.rate[, 1], type = "l", xlab = "Trees", ylab = "error") #error
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=300)
#prediction
new_test_rf = df_test_d[,(names(df_test_d) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
varImpPlot(forest)
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
fit_rf = randomForest(RainTomorrow~., data=df_train_cat)
peso_variables_rf = importance(fit_rf)
feat_imp_df <- importance(fit_rf) %>%
data.frame() %>%
mutate(feature = row.names(.))
variables_seleccionadas = feat_imp_df %>% filter(MeanDecreaseGini> 80)
variables_rf = variables_seleccionadas[['feature']]
varImpPlot(fit_rf)
print(variables_rf)
new_train_rf = df_train_cat[,(names(df_train_cat) %in% c(variables_rf,'RainTomorrow'))]
down_train_rf = downsample(new_train_rf)
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=400)
plot(forest$err.rate[, 1], type = "l", xlab = "Trees", ylab = "error") #error
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=300)
#prediction
new_test_rf = df_test_cat[,(names(df_test_cat) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
varImpPlot(forest)
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
df_train_cat = df_train
df_train_cat$Season <- as.numeric(as.factor(df_train_cat$Season))
df_train_cat$Location <- as.numeric(as.factor(df_train_cat$Location))
df_train_cat$WindGustDir <- as.numeric(as.factor(df_train_cat$WindGustDir))
df_train_cat$WindDir9am <- as.numeric(as.factor(df_train_cat$WindDir9am))
df_train_cat$WindDir3pm <- as.numeric(as.factor(df_train_cat$WindDir3pm))
df_test_cat = df_test
df_test_cat$Season <- as.numeric(as.factor(df_test_cat$Season))
df_test_cat$Location <- as.numeric(as.factor(df_test_cat$Location))
df_test_cat$WindGustDir <- as.numeric(as.factor(df_test_cat$WindGustDir))
df_test_cat$WindDir9am <- as.numeric(as.factor(df_test_cat$WindDir9am))
df_test_cat$WindDir3pm <- as.numeric(as.factor(df_test_cat$WindDir3pm))
fit_rf = randomForest(RainTomorrow~., data=df_train_cat)
peso_variables_rf = importance(fit_rf)
feat_imp_df <- importance(fit_rf) %>%
data.frame() %>%
mutate(feature = row.names(.))
variables_seleccionadas = feat_imp_df %>% filter(MeanDecreaseGini> 80)
variables_rf = variables_seleccionadas[['feature']]
varImpPlot(fit_rf)
print(variables_rf)
new_train_rf = df_train_cat[,(names(df_train_cat) %in% c(variables_rf,'RainTomorrow'))]
down_train_rf = downsample(new_train_rf)
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=400)
plot(forest$err.rate[, 1], type = "l", xlab = "Trees", ylab = "error") #error
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=300)
#prediction
new_test_rf = df_test_cat[,(names(df_test_cat) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
varImpPlot(forest)
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=1000)
plot(forest$err.rate[, 1], type = "l", xlab = "Trees", ylab = "error") #error
#prediction
new_test_rf = df_test_cat[,(names(df_test_cat) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
varImpPlot(forest)
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=500)
#prediction
new_test_rf = df_test_cat[,(names(df_test_cat) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
ggplot(data=resultados_knn,aes(x=index,y=accuracy)) +
geom_col(colour="cyan4")+
ggtitle("Accuracy")
#LINEAR
set.seed(1)
tune.out = tune(svm, RainTomorrow ~ ., data = down_train, kernel = c("linear"), ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10, 100)))
ggplot(data=resultados_knn,aes(x=index,y=accuracy)) +
geom_col()+
ggtitle("Accuracy")
library(class)
library(cluster)
library(dplyr)
library(corrplot)
library(ggplot2)
library(MASS)
library(ggfortify)
library(Rtsne)
library(ggpubr)
library(ggmosaic)
library(tidyverse)
library(GGally)
library(mice)
library(randomForest)
library(stringi)
library(fastDummies)
library(caret)
library(glmnet)
library(ROCR)
library(magrittr)
library(tidyr)
library(vcd)
library(InformationValue)
library(grid)
library(party)
library(rpart)
library(ipred)
library(distances)
library(InformationValue)
library(e1071)
library(naivebayes)
library(rpart.plot)
library(pROC)
library(ROCit)
setwd("~/Escritorio/MASTER/CUATRI2/MACHINE LEARNING I/PRACTICA/australian_rain_ML1")
ggplot(data=resultados_knn,aes(x=index,y=accuracy)) +
geom_col()+
ggtitle("Accuracy")
#ggplot(data=resultados_knn,aes(x=index,y=recall)) +
#  geom_col(colour="darkblue",fill="blue") +
#  ggtitle("Recall values")
ree <- read.csv("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ree.csv")
View(ree)
vis_dat(df_ree)
(cols_withNa <- apply(df_ree, 2, function(x) sum(is.na(x))))
df_ree <- read.csv("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ree.csv")
vis_dat(df_ree)
(cols_withNa <- apply(df_ree, 2, function(x) sum(is.na(x))))
library(tidyverse)
library(tsibble)
library(fable)
library(feast)
library(feasts)
library(tsibbledata)
library(normtest)
df_demanda <- df_ree %>%
filter(subcategory == "Demanda en b.c.")
df_demanda %>%
autoplot(value) +
labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney") +
xlab("Year")
# Transform into a tibble object
df_ree <- df_ree %>%
mutate(quarter = yearquarter(datetime)) %>%
as_tsibble(key = c(state, gender, legal, indigenous), index = quarter)
df_ree$datetime <- strptime(x = as.character(df_ree$datetime),
format = "%d/%m/%Y %H:%M")
df_ree <- df_ree %>%
#  mutate(quarter = yearquarter(datetime)) %>%
as_tsibble(key = c(state, gender, legal, indigenous), index = datetime)
df_ree <- df_ree %>%
#  mutate(quarter = yearquarter(datetime)) %>%
as_tsibble(key = c(value, subcategory, percentage), index = datetime)
df_ree <- df_ree %>%
as_tsibble(key = c(value, subcategory, percentage), index = datetime)
df_ree <- read.csv("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ree.csv")
df_ree$datetime <- strptime(x = as.character(df_ree$datetime),
format = "%d/%m/%Y %H:%M")
df_ree <- df_ree %>%
as_tsibble(key = c(energy, subcategory, percentage), index = datetime)
df_ree$datetime <- as.POSIXct(df_ree$datetime)
df_ree <- df_ree %>%
as_tsibble(key = c(energy, subcategory, percentage), index = datetime)
#check null values
(cols_withNa <- apply(df_ree, 2, function(x) sum(is.na(x))))
df_ree <- read.csv("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ree.csv")
df_ree$datetime <- as.POSIXct(df_ree$datetime)
df_ree <- df_ree %>%
as_tsibble(key = c(energy, subcategory, percentage), index = datetime)
df_demanda <- df_ree %>%
filter(subcategory == "Demanda en b.c.")
df_demanda %>%
autoplot(value) +
labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney") +
xlab("Year")
df_demanda %>%
autoplot(energy) +
labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney") +
xlab("Year")
no
df_demanda %>%
autoplot(energy) +
labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney")
n
df_ree <- df_ree %>%
as_tsibble(key = c(energy), index = datetime)
df_demanda <- df_ree %>%
filter(subcategory == "Demanda en b.c.")
df_demanda %>%
autoplot(energy) +
labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney")
df_demanda <- df_ree %>%
filter(subcategory == "Demanda en b.c.")
df_demanda <- df_demanda %>%
as_tsibble(key = c(energy, subcategory, percentage), index = datetime)
df_demanda %>%
autoplot(energy) +
labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney")
n
df_demanda <- df_demanda %>%
as_tsibble(key = c(energy), index = datetime)
df_demanda1 <- df_demanda %>%
as_tsibble(key = c(energy), index = datetime)
library(tidyverse)
library(tsibble)
library(fable)
library(feasts)
library(tsibbledata)
library(normtest)
setwd("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ML2")
df_ree <- read.csv("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ML2/ree.csv")
#df_ree$datetime <- strptime(x = as.character(df_ree$datetime),
#                                format = "%d/%m/%Y %H:%M")
df_ree$id <- NULL
df_ree$date = substr(df_ree$datetime,1,10)
#check null values
(cols_withNa <- apply(df_ree, 2, function(x) sum(is.na(x))))
df_demanda <- df_ree %>%
filter(subcategory == "Demanda en b.c.")
#df_demanda$datetime <- as.POSIXct(df_demanda$datetime)
df_demanda$date <- as.Date(df_demanda$datetime, format="%d/%m/%Y")
# Transform into a tibble object
df_demanda1 <- df_demanda %>%
as_tsibble(index = date) #key = c(energy), index = datetime
df_demanda$datetime <- as.POSIXct(df_demanda$datetime)
df_demanda$date <- as.Date(df_demanda$datetime, format="%d/%m/%Y")
# Transform into a tibble object
df_demanda1 <- df_demanda %>%
as_tsibble(index = date) #key = c(energy), index = datetime
# plot series temporales
df_demanda1 %>%
autoplot(energy) +
labs(title = "", subtitle = "")
#Calcular la diferenciación
df_demanda1 %>% plot (diff ((energy)),
main=" ")
#Calcular la diferenciación
df_demanda1 %>% plot (diff(energy), main=" ")
library(tidyverse)
library(tsibble)
library(fable)
library(feasts)
library(tsibbledata)
library(normtest)
setwd("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ML2")
df_ree <- read.csv("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ML2/ree.csv")
#df_ree$datetime <- strptime(x = as.character(df_ree$datetime),
#                                format = "%d/%m/%Y %H:%M")
df_ree$id <- NULL
df_ree$date = substr(df_ree$datetime,1,10)
#check null values
(cols_withNa <- apply(df_ree, 2, function(x) sum(is.na(x))))
df_demanda <- df_ree %>%
filter(subcategory == "Demanda en b.c.")
df_demanda$datetime <- as.POSIXct(df_demanda$datetime)
df_demanda$date <- as.Date(df_demanda$datetime, format="%d/%m/%Y")
# Transform into a tibble object
df_demanda1 <- df_demanda %>%
as_tsibble(index = date) #key = c(energy), index = datetime
# plot series temporales
df_demanda1 %>%
autoplot(energy) +
labs(title = "", subtitle = "")
# Correlograma
df_demanda1 %>% ACF(energy) %>% autoplot()
# Descomposición clásica
df_demanda1 %>%
model(classical_decomposition(energy, type = "additive")) %>%
components() %>%
autoplot() + xlab(" ") +
ggtitle(" ")
# Plot the serie
# Se observa que tras la diferenciación la serie pierde la tendencia, siendo claramente estacionaria en media.
df_demanda1 %>% autoplot( difference(energy)) +
xlab("  ") + ylab(" ") +
ggtitle("  ")
#Comprobar si existen autocorrelaciones significaticas:
df_demanda1 %>%
mutate(diff_close = difference(energy)) %>%
features(diff_close, ljung_box, lag = 10)
#Calcular la diferenciación
df_demanda1 %>% autoplot (diff(energy), main=" ")
# Plot the serie
# Se observa que tras la diferenciación la serie pierde la tendencia, siendo claramente estacionaria en media.
df_demanda1 %>% autoplot( difference(energy)) +
xlab("  ") + ylab(" ") +
ggtitle("  ")
df_demanda1 %>%
model(classical_decomposition(energy, type = "multiplicative")) %>%
components() %>%
autoplot() + xlab(" ") +
ggtitle(" ")
# Descomposición clásica
df_demanda1 %>%
model(classical_decomposition(energy, type = "additive")) %>%
components() %>%
autoplot() + xlab(" ") +
ggtitle(" ")
# Correlograma
df_demanda1 %>% ACF(energy) %>% autoplot()
#Autocorrelación parcial
df_demanda1 %>% PACF(energy) %>% autoplot()
# Correlograma
# Gráfico en el que representamos los valores de la función de autocorrelación empírica r k contra los retardos k = 1, 2, ... , M donde típicamente M ≪ N .
# Vemos que  tiene muchos componentes significativas para algún retardo.
df_demanda1 %>% ACF(energy) %>% autoplot()
#Autocorrelación parcial
# correlación que resulta después de eliminar el efecto de cualquier correlación en retardos más cortos
# autocorrelación entre los instantes t y t + k condicionada a  los valores que toma la serie en los instantes t + 1, t + 2, ... , t + k − 1 .
df_demanda1 %>% PACF(energy) %>% autoplot()
demanda_decomposed = classical_decomposition(energy, type = "additive")
View(demanda_decomposed)
#AR
x_armod = ar (df_demanda1, method = "mle") # Estimación por máxima verosimilitud
#ARIMA
elec_ts_mod = arima (df_demanda1$energy, order = c (2,1,2))
elec_ts_mod
elec_ts_mod %>% my_tsresiduals()
#SARIMA
fit <- df_demanda1$energy %>%
model(arima = ARIMA(value ~ pdq(0,1,1) + PDQ(0,1,1)))
#SARIMA
fit <- model(arima = ARIMA(df_demanda1$energy ~ pdq(0,1,1) + PDQ(0,1,1)))
#SARIMA
fit <- df_demanda1 %>%
model(arima = ARIMA(energy ~ pdq(0,1,1) + PDQ(0,1,1)))
fit %>% my_tsresiduals()
#predicción
fit %>% forecast(h=12) %>% autoplot(df_demanda1)
