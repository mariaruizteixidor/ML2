cm_tree$overall[1],
cm_knn$overall[1],
cm_svm$overall[1],
cm_nb$overall[1]))
ggplot(aes(x=Model, y=Accuracy), data=model_compare) +
geom_bar(stat='identity') +
ggtitle('Comparación del accuracy según cada modelo') +
xlab('Models') +
ylab('Overall Accuracy')
espec_svm = cm_svm$table[2,2]/(cm_svm$table[2,2] + cm_svm$table[1,2])
espec_glm = cm_glm$table[2,2]/(cm_glm$table[2,2] + cm_glm$table[1,2])
espec_svm = cm_svm$table[2,2]/(cm_svm$table[2,2] + cm_svm$table[1,2])
espec_tree = cm_tree$table[2,2]/(cm_tree$table[2,2] + cm_tree$table[1,2])
espec_rf = cm_rf$table[2,2]/(cm_rf$table[2,2] + cm_rf$table[1,2])
espec_knn = cm_knn$table[2,2]/(cm_knn$table[2,2] + cm_knn$table[1,2])
espec_nb = cm_nb$table[2,2]/(cm_nb$table[2,2] + cm_nb$table[1,2])
model_compare <- data.frame(Model = c('RegresiónLogística',
'RF',
'Decision Tree',
'KNN',
'SVM',
'NaiveBayes '),
sensibilidad = c(espec_glm,
espec_rf,
espec_tree,
espec_knn,
espec_svm,
espec_nb))
ggplot(aes(x=Model, y=sensibilidad), data=model_compare) +
geom_bar(stat='identity') +
ggtitle('Comparación de la Sensibilidad según cada modelo') +
xlab('Models') +
ylab('Overall Recall')
model_compare
Precision_glm = cm_glm$table[1,1]/(cm_glm$table[1,1] + cm_glm$table[2,1])
Precision_svm = cm_svm$table[1,1]/(cm_svm$table[1,1] + cm_svm$table[2,1])
Precision_tree = cm_tree$table[1,1]/(cm_tree$table[1,1] + cm_tree$table[2,1])
Precision_rf = cm_rf$table[1,1]/(cm_rf$table[1,1] + cm_rf$table[2,1])
Precision_knn = cm_knn$table[1,1]/(cm_knn$table[1,1] + cm_knn$table[2,1])
Precision_nb = cm_nb$table[1,1]/(cm_nb$table[1,1] + cm_nb$table[2,1])
Fscore_glm = (2*Precision_glm*espec_glm) / sum(Precision_glm, espec_glm)
Fscore_svm = (2*Precision_svm*espec_svm) / sum(Precision_svm, espec_svm)
Fscore_tree = (2*Precision_tree*espec_tree) / sum(Precision_tree, espec_tree)
Fscore_rf = (2*Precision_rf*espec_rf) / sum(Precision_rf, espec_rf)
Fscore_knn = (2*Precision_knn*espec_knn) / sum(Precision_knn, espec_knn)
Fscore_nb = (2*Precision_nb*espec_nb) / sum(Precision_nb, espec_nb)
model_compare <- data.frame(Model = c('RegresiónLogística',
'RF',
'Decision Tree',
'KNN',
'SVM',
'NaiveBayes '),
F1 = c(Fscore_glm,
Fscore_rf,
Fscore_tree,
Fscore_knn,
Fscore_svm,
Fscore_nb))
ggplot(aes(x=Model, y=F1), data=model_compare) +
geom_bar(stat='identity') +
ggtitle('Comparación de F1 score según cada modelo') +
xlab('Models') +
ylab('Overall F1')
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=200)
plot(forest$err.rate[, 1], type = "l", xlab = "Trees", ylab = "error") #error
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=200)
#prediction
new_test_rf = df_test_cat[,(names(df_test_cat) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
varImpPlot(forest)
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=400)
plot(forest$err.rate[, 1], type = "l", xlab = "Trees", ylab = "error") #error
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=300)
#prediction
new_test_rf = df_test_cat[,(names(df_test_cat) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
varImpPlot(forest)
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
fit_rf = randomForest(RainTomorrow~., data=df_train_d)
peso_variables_rf = importance(fit_rf)
feat_imp_df <- importance(fit_rf) %>%
data.frame() %>%
mutate(feature = row.names(.))
variables_seleccionadas = feat_imp_df %>% filter(MeanDecreaseGini> 100)
variables_rf = variables_seleccionadas[['feature']]
varImpPlot(fit_rf)
print(variables_rf)
new_train_rf = df_train_d[,(names(df_train_d) %in% c(variables_rf,'RainTomorrow'))]
down_train_rf = downsample(new_train_rf)
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=400)
plot(forest$err.rate[, 1], type = "l", xlab = "Trees", ylab = "error") #error
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=300)
#prediction
new_test_rf = df_test_d[,(names(df_test_d) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
varImpPlot(forest)
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
fit_rf = randomForest(RainTomorrow~., data=df_train_cat)
peso_variables_rf = importance(fit_rf)
feat_imp_df <- importance(fit_rf) %>%
data.frame() %>%
mutate(feature = row.names(.))
variables_seleccionadas = feat_imp_df %>% filter(MeanDecreaseGini> 80)
variables_rf = variables_seleccionadas[['feature']]
varImpPlot(fit_rf)
print(variables_rf)
new_train_rf = df_train_cat[,(names(df_train_cat) %in% c(variables_rf,'RainTomorrow'))]
down_train_rf = downsample(new_train_rf)
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=400)
plot(forest$err.rate[, 1], type = "l", xlab = "Trees", ylab = "error") #error
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=300)
#prediction
new_test_rf = df_test_cat[,(names(df_test_cat) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
varImpPlot(forest)
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
df_train_cat = df_train
df_train_cat$Season <- as.numeric(as.factor(df_train_cat$Season))
df_train_cat$Location <- as.numeric(as.factor(df_train_cat$Location))
df_train_cat$WindGustDir <- as.numeric(as.factor(df_train_cat$WindGustDir))
df_train_cat$WindDir9am <- as.numeric(as.factor(df_train_cat$WindDir9am))
df_train_cat$WindDir3pm <- as.numeric(as.factor(df_train_cat$WindDir3pm))
df_test_cat = df_test
df_test_cat$Season <- as.numeric(as.factor(df_test_cat$Season))
df_test_cat$Location <- as.numeric(as.factor(df_test_cat$Location))
df_test_cat$WindGustDir <- as.numeric(as.factor(df_test_cat$WindGustDir))
df_test_cat$WindDir9am <- as.numeric(as.factor(df_test_cat$WindDir9am))
df_test_cat$WindDir3pm <- as.numeric(as.factor(df_test_cat$WindDir3pm))
fit_rf = randomForest(RainTomorrow~., data=df_train_cat)
peso_variables_rf = importance(fit_rf)
feat_imp_df <- importance(fit_rf) %>%
data.frame() %>%
mutate(feature = row.names(.))
variables_seleccionadas = feat_imp_df %>% filter(MeanDecreaseGini> 80)
variables_rf = variables_seleccionadas[['feature']]
varImpPlot(fit_rf)
print(variables_rf)
new_train_rf = df_train_cat[,(names(df_train_cat) %in% c(variables_rf,'RainTomorrow'))]
down_train_rf = downsample(new_train_rf)
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=400)
plot(forest$err.rate[, 1], type = "l", xlab = "Trees", ylab = "error") #error
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=300)
#prediction
new_test_rf = df_test_cat[,(names(df_test_cat) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
varImpPlot(forest)
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=1000)
plot(forest$err.rate[, 1], type = "l", xlab = "Trees", ylab = "error") #error
#prediction
new_test_rf = df_test_cat[,(names(df_test_cat) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
varImpPlot(forest)
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
forest = randomForest(RainTomorrow~., data=down_train_rf, ntree=500)
#prediction
new_test_rf = df_test_cat[,(names(df_test_cat) %in% c(variables_rf,'RainTomorrow'))]
pred_forest<-predict(forest, newdata=new_test_rf, type="response")
best_prediction_rf <- predict(forest, newdata = new_test_rf, type = "prob")
conf_table = table(pred_forest, new_test_rf$RainTomorrow)
print(caret::confusionMatrix(conf_table, positive = '1',  mode = "prec_recall"))
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
ggplot(data=resultados_knn,aes(x=index,y=accuracy)) +
geom_col(colour="cyan4")+
ggtitle("Accuracy")
#LINEAR
set.seed(1)
tune.out = tune(svm, RainTomorrow ~ ., data = down_train, kernel = c("linear"), ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10, 100)))
ggplot(data=resultados_knn,aes(x=index,y=accuracy)) +
geom_col()+
ggtitle("Accuracy")
library(class)
library(cluster)
library(dplyr)
library(corrplot)
library(ggplot2)
library(MASS)
library(ggfortify)
library(Rtsne)
library(ggpubr)
library(ggmosaic)
library(tidyverse)
library(GGally)
library(mice)
library(randomForest)
library(stringi)
library(fastDummies)
library(caret)
library(glmnet)
library(ROCR)
library(magrittr)
library(tidyr)
library(vcd)
library(InformationValue)
library(grid)
library(party)
library(rpart)
library(ipred)
library(distances)
library(InformationValue)
library(e1071)
library(naivebayes)
library(rpart.plot)
library(pROC)
library(ROCit)
setwd("~/Escritorio/MASTER/CUATRI2/MACHINE LEARNING I/PRACTICA/australian_rain_ML1")
ggplot(data=resultados_knn,aes(x=index,y=accuracy)) +
geom_col()+
ggtitle("Accuracy")
#ggplot(data=resultados_knn,aes(x=index,y=recall)) +
#  geom_col(colour="darkblue",fill="blue") +
#  ggtitle("Recall values")
ree <- read.csv("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ree.csv")
View(ree)
vis_dat(df_ree)
(cols_withNa <- apply(df_ree, 2, function(x) sum(is.na(x))))
df_ree <- read.csv("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ree.csv")
vis_dat(df_ree)
(cols_withNa <- apply(df_ree, 2, function(x) sum(is.na(x))))
library(tidyverse)
library(tsibble)
library(fable)
library(feast)
library(feasts)
library(tsibbledata)
library(normtest)
df_demanda <- df_ree %>%
filter(subcategory == "Demanda en b.c.")
df_demanda %>%
autoplot(value) +
labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney") +
xlab("Year")
# Transform into a tibble object
df_ree <- df_ree %>%
mutate(quarter = yearquarter(datetime)) %>%
as_tsibble(key = c(state, gender, legal, indigenous), index = quarter)
df_ree$datetime <- strptime(x = as.character(df_ree$datetime),
format = "%d/%m/%Y %H:%M")
df_ree <- df_ree %>%
#  mutate(quarter = yearquarter(datetime)) %>%
as_tsibble(key = c(state, gender, legal, indigenous), index = datetime)
df_ree <- df_ree %>%
#  mutate(quarter = yearquarter(datetime)) %>%
as_tsibble(key = c(value, subcategory, percentage), index = datetime)
df_ree <- df_ree %>%
as_tsibble(key = c(value, subcategory, percentage), index = datetime)
df_ree <- read.csv("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ree.csv")
df_ree$datetime <- strptime(x = as.character(df_ree$datetime),
format = "%d/%m/%Y %H:%M")
df_ree <- df_ree %>%
as_tsibble(key = c(energy, subcategory, percentage), index = datetime)
df_ree$datetime <- as.POSIXct(df_ree$datetime)
df_ree <- df_ree %>%
as_tsibble(key = c(energy, subcategory, percentage), index = datetime)
#check null values
(cols_withNa <- apply(df_ree, 2, function(x) sum(is.na(x))))
df_ree <- read.csv("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ree.csv")
df_ree$datetime <- as.POSIXct(df_ree$datetime)
df_ree <- df_ree %>%
as_tsibble(key = c(energy, subcategory, percentage), index = datetime)
df_demanda <- df_ree %>%
filter(subcategory == "Demanda en b.c.")
df_demanda %>%
autoplot(value) +
labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney") +
xlab("Year")
df_demanda %>%
autoplot(energy) +
labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney") +
xlab("Year")
no
df_demanda %>%
autoplot(energy) +
labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney")
n
df_ree <- df_ree %>%
as_tsibble(key = c(energy), index = datetime)
df_demanda <- df_ree %>%
filter(subcategory == "Demanda en b.c.")
df_demanda %>%
autoplot(energy) +
labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney")
df_demanda <- df_ree %>%
filter(subcategory == "Demanda en b.c.")
df_demanda <- df_demanda %>%
as_tsibble(key = c(energy, subcategory, percentage), index = datetime)
df_demanda %>%
autoplot(energy) +
labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney")
n
df_demanda <- df_demanda %>%
as_tsibble(key = c(energy), index = datetime)
df_demanda1 <- df_demanda %>%
as_tsibble(key = c(energy), index = datetime)
library(tidyverse)
library(tsibble)
library(fable)
library(feasts)
library(tsibbledata)
library(normtest)
library(lmtest)
library(forecast)
setwd("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ML2")
df_ree <- read.csv("~/Escritorio/MASTER/CUATRI3/MACHINE_LEARNING2/PRACTICA/ML2/ree.csv")
#df_ree$datetime <- strptime(x = as.character(df_ree$datetime),
#                                format = "%d/%m/%Y %H:%M")
df_ree$id <- NULL
df_ree$date = substr(df_ree$datetime,1,10)
#check null values
(cols_withNa <- apply(df_ree, 2, function(x) sum(is.na(x))))
df_demanda <- df_ree %>%
filter(subcategory == "Demanda en b.c.")
df_demanda$datetime <- as.POSIXct(df_demanda$datetime)
df_demanda$date <- as.Date(df_demanda$datetime, format="%d/%m/%Y")
# dividir TRAIN - TEST
# Train: enero 2018- enero 2020
df_demanda_train = df_demanda[df_demanda$date < '2020-02-01',]
# Test: febrero 2020
df_demanda_test = df_demanda[(df_demanda$date >= '2020-02-01') & (df_demanda$date < '2020-03-01'),]
# Transform into a tibble object
df_demanda_train <- df_demanda_train %>%
as_tsibble(index = date) #key = c(energy), index = datetime
df_demanda_test <- df_demanda_test %>%
as_tsibble(index = date) #key = c(energy), index = datetime# Descomposición clásica
df_demanda <- df_demanda %>%
as_tsibble(index = date) #key = c(energy), index = datetime# Descomposición clásica
# plot series temporales
df_demanda_train %>%
autoplot(energy) +
labs(title = "", subtitle = "")
#aditivo
df_demanda_train %>%
model(classical_decomposition(energy, type = "additive")) %>%
components() %>%
autoplot() + xlab(" ") +
ggtitle(" ")
demanda_decomposed = classical_decomposition(energy, type = "additive")
#multiplicativo
df_demanda_train %>%
model(classical_decomposition(energy, type = "multiplicative")) %>%
components() %>%
autoplot() + xlab(" ") +
ggtitle(" ")
# Diferenciación
# Se observa que tras la diferenciación la serie pierde la tendencia, siendo claramente estacionaria en media.
# yt = xt - x(t-1)
df_demanda_train %>% autoplot( difference(energy)) +
xlab("  ") + ylab(" ") +
ggtitle("  ")
# Una forma de determinar más objetivamente si es necesario diferenciar es utilizar una prueba de raíz unitaria.
# En este contraste, la hipótesis nula es que los datos son estacionarios, y buscamos pruebas de que la hipótesis nula es falsa. En consecuencia, pequeños p-valores (por ejemplo, menos de 0,05) sugieren que es necesario diferenciar.
df_demanda_train %>%
features(energy, unitroot_kpss)
df_demanda_train %>%
mutate(diff_close = difference(energy)) %>%
features(diff_close, unitroot_kpss)
#Comprobar si existen autocorrelaciones significaticas:
df_demanda_train %>%
mutate(diff_close = difference(energy)) %>%
features(diff_close, ljung_box, lag = 10)
# MODELOS
# es posible utilizar las gráfica de ACF y PACF, para determinar los valores apropiados para p y q.
# Correlograma
# Gráfico en el que representamos los valores de la función de autocorrelación empírica r k contra los retardos k = 1, 2, ... , M donde típicamente M ≪ N .
# Vemos que  tiene muchos componentes significativas para algún retardo.
df_demanda_train %>% ACF(difference(energy, 1)) %>% autoplot()
df_demanda_train %>% ACF(energy) %>% autoplot()
# correlación que resulta después de eliminar el efecto de cualquier correlación en retardos más cortos
# autocorrelación entre los instantes t y t + k condicionada a  los valores que toma la serie en los instantes t + 1, t + 2, ... , t + k − 1 .
df_demanda_train %>% PACF(difference(energy, 1)) %>% autoplot()
df_demanda_train %>% PACF(energy) %>% autoplot()
df_demanda1_diff <- difference(df_demanda_train$energy, 7)
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
fit <- df_demanda_train %>%
model(arima = ARIMA(energy))
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Primero probamos con un modelo sin ajustar, automático:
fit <- df_demanda_train %>%
model(arima = ARIMA(energy))
report(fit)
fit %>% gg_tsresiduals()
#Si visualizamos los residuos, vemos que aún hay picos que hacen que no sea ruido blanco todavía y un pico en el ACF en el retardo 14.
fit %>% gg_tsresiduals()
df_demanda_train$energy %>% gg_tsdisplay()
#predicción
fit %>% forecast(h=15) %>% autoplot(df_demanda_train)
# Ajuste 2
fit <- df_demanda_train %>%
model(arima = ARIMA(energy~ pdq(1,0,2) + PDQ(1,0,1)))
# Ajuste 2
fit <- df_demanda_train %>%
model(arima = ARIMA(energy~ pdq(1,0,2) + PDQ(1,0,1)))
# Ajuste 2
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(0,1,1) + PDQ(0,1,1)))
# Ajuste 2
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(1,0,2) + PDQ(1,0,1)))
# MODELOS
# es posible utilizar las gráfica de ACF y PACF, para determinar los valores apropiados para p y q.
# Correlograma
# Gráfico en el que representamos los valores de la función de autocorrelación empírica r k contra los retardos k = 1, 2, ... , M donde típicamente M ≪ N .
# Vemos que  tiene muchos componentes significativas para algún retardo.
#df_demanda_train %>% ACF(difference(energy, 1)) %>% autoplot()
# Mejor observar el ACF y PACF sin la diferenciación primero
df_demanda_train %>% ACF(energy) %>% autoplot()
# correlación que resulta después de eliminar el efecto de cualquier correlación en retardos más cortos
# autocorrelación entre los instantes t y t + k condicionada a  los valores que toma la serie en los instantes t + 1, t + 2, ... , t + k − 1 .
#df_demanda_train %>% PACF(difference(energy, 1)) %>% autoplot()
# Mejor observar el ACF y PACF sin la diferenciación primero
df_demanda_train %>% PACF(energy) %>% autoplot()
# correlación que resulta después de eliminar el efecto de cualquier correlación en retardos más cortos
# autocorrelación entre los instantes t y t + k condicionada a  los valores que toma la serie en los instantes t + 1, t + 2, ... , t + k − 1 .
#df_demanda_train %>% PACF(difference(energy, 1)) %>% autoplot()
# Mejor observar el ACF y PACF sin la diferenciación primero
df_demanda_train %>% PACF(energy) %>% autoplot()
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(1,0,1) + PDQ(1,0,1)))
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(2,0,1) + PDQ(1,0,1)))
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(0,0,1) + PDQ(0,0,1)))
report(fit)
fit %>% gg_tsresiduals()
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(0,0,2) + PDQ(0,0,2)))
report(fit)
fit %>% gg_tsresiduals()
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(0,0,2) + PDQ(0,1,2))) #pdq(0,0,2) + PDQ(0,0,2)
report(fit)
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(0,0,2) + PDQ(0,1,1))) #pdq(0,0,2) + PDQ(0,0,2)
report(fit)
fit %>% gg_tsresiduals()
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(0,1,2) + PDQ(0,0,2))) #pdq(0,0,2) + PDQ(0,0,2)
report(fit)
fit %>% gg_tsresiduals()
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(0,0,2) + PDQ(1,0,2))) #pdq(0,0,2) + PDQ(0,0,2)
report(fit)
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(0,0,2) + PDQ(1,0,1))) #pdq(0,0,2) + PDQ(0,0,2)
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(1,0,2) + PDQ(0,0,2))) #pdq(0,0,2) + PDQ(0,0,2)
report(fit)
fit %>% gg_tsresiduals()
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(1,0,2) + PDQ(0,0,3))) #pdq(0,0,2) + PDQ(0,0,2)
report(fit)
fit %>% gg_tsresiduals()
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(2,0,2) + PDQ(0,0,2))) #pdq(0,0,2) + PDQ(0,0,2)
report(fit)
fit %>% gg_tsresiduals()
# SARIMA ---------------------------------------------------------------------------------------------------------------------------------
# Ajuste 1
# Probamos ajustando un modelo con componentes de
fit <- df_demanda_train %>%
model(arima = ARIMA(energy ~ pdq(0,0,2) + PDQ(0,0,2))) #pdq(0,0,2) + PDQ(0,0,2)
report(fit)
fit %>% gg_tsresiduals()
